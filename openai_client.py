import openai
import os
from dotenv import load_dotenv
import logging
from transformers import AutoTokenizer, AutoModel
import torch

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("openai_client")

# Load environment variables from a .env file
load_dotenv()

# Initialize the OpenAI client
client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

# Load pre-trained model and tokenizer for embeddings
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

def ask_openai(context: str) -> str:
    """
    Send a context to the OpenAI API and return the generated response.
    
    Args:
        context (str): The context or prompt to send to the OpenAI API.
    
    Returns:
        str: The response generated by the OpenAI API.
    """
    try:
        logger.info(f"Sending context to OpenAI: {context}")

        # Use the correct API method for generating chat completions
        response = client.chat.completions.create(
            model="gpt-4",  # or "gpt-3.5-turbo", based on your access
            messages=[
                {"role": "system", "content": "You are an SQL query generator."},
                {"role": "user", "content": context},
            ],
            max_tokens=150,  # Adjust based on expected length of the SQL query
            n=1,
            stop=None,
            temperature=0.5,
        )

        # Correctly access the generated SQL query
        sql_query = response.choices[0].message.content.strip()
        logger.info(f"Generated SQL query: {sql_query}")
        return sql_query

    except Exception as e:
        logger.error(f"Error while communicating with OpenAI: {str(e)}")
        raise

def get_embeddings(text_list):
    """
    Generate embeddings for a list of text strings using a pre-trained model.
    
    Args:
        text_list (list of str): List of text strings to generate embeddings for.
    
    Returns:
        torch.Tensor: The embeddings for the input text list.
    """
    try:
        logger.info(f"Generating embeddings for text: {text_list}")

        # Tokenize and get embeddings
        tokens = tokenizer(text_list, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            embeddings = model(**tokens).last_hidden_state.mean(dim=1)

        logger.info(f"Generated embeddings: {embeddings}")
        return embeddings

    except Exception as e:
        logger.error(f"Error generating embeddings: {str(e)}")
        raise

def find_best_match(query_embedding, embedding_list):
    """
    Find the best match from a list of embeddings for a given query embedding.
    
    Args:
        query_embedding (torch.Tensor): The embedding of the query.
        embedding_list (list of torch.Tensor): The list of embeddings to match against.
    
    Returns:
        str: The best matching text from the list.
        float: The similarity score of the best match.
    """
    try:
        logger.info("Finding best match for the query embedding")

        # Calculate cosine similarity
        similarities = torch.nn.functional.cosine_similarity(query_embedding, embedding_list)
        best_match_idx = similarities.argmax().item()
        best_match_score = similarities[best_match_idx].item()

        logger.info(f"Best match index: {best_match_idx}, similarity score: {best_match_score}")
        return best_match_idx, best_match_score

    except Exception as e:
        logger.error(f"Error finding best match: {str(e)}")
        raise
